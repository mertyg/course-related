{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MERT YUKSEKGONUL\n",
    "### 2016402147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTtOiRzYixSp"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfpGY2GF9JtS"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNW3cQLwRURz"
   },
   "source": [
    "I have trained the model on Donald Trump's speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y4T1fSSivtr"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  # Subtract max for stabilization, preventing overflow etc.  \n",
    "  x_s = x-np.max(x)\n",
    "  return np.exp(x_s) / np.sum(np.exp(x_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7B1p9vuMv59q"
   },
   "outputs": [],
   "source": [
    "# I will use the single layered vanilla char-rnn model, with adam optimizer.\n",
    "\n",
    "class CharRNN:\n",
    "  def __init__(self, d_hidden, d_vocab, maps, seq_len = 25, w_init=0.01, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-08):\n",
    "    \"\"\"\n",
    "    Character predictor RNN model. Author: Mert Yuksekgonul (@mertyg)\n",
    "    d_hidden, d_vocab : Hidden size and vocab size, int\n",
    "    maps : A list of maps. First one is expected to be a char to index map, second one is the reverse index to char map.\n",
    "    seq_len : Length of windows to be considered\n",
    "    w_init : Initialization constant for weight matrices.\n",
    "    learning_rate : lr for adam.\n",
    "    beta1, beta2, epsilon: other parameters for adam optimization\n",
    "    \"\"\"\n",
    "    self.d_h = d_hidden\n",
    "    self.d_v = d_vocab\n",
    "    np.random.seed(0)\n",
    "    self.Whx = np.random.randn(d_hidden, d_vocab)*w_init\n",
    "    self.Whh = np.random.randn(d_hidden, d_hidden)*w_init\n",
    "    self.Wyh = np.random.randn(d_vocab, d_hidden)*w_init\n",
    "    self.bh = np.zeros((d_hidden, 1))\n",
    "    self.by = np.zeros((d_vocab, 1))\n",
    "    self.lr = learning_rate\n",
    "    self.beta1 = beta1\n",
    "    self.beta2 = beta2\n",
    "    self.epsilon = epsilon\n",
    "    self.char_ix = maps[0]\n",
    "    self.ix_char = maps[1]\n",
    "    self.seq_len = 25\n",
    "    self.mWyh, self.vWyh = np.zeros_like(self.Wyh), np.zeros_like(self.Wyh)\n",
    "    self.mWhx, self.vWhx = np.zeros_like(self.Whx), np.zeros_like(self.Whx)\n",
    "    self.mWhh, self.vWhh = np.zeros_like(self.Whh), np.zeros_like(self.Whh)\n",
    "    self.mby, self.vby = np.zeros_like(self.by), np.zeros_like(self.by)\n",
    "    self.mbh, self.vbh = np.zeros_like(self.bh), np.zeros_like(self.bh)\n",
    "  \n",
    "  def _forward_cell(self, h_prev, x):\n",
    "    h_next = np.tanh(np.dot(self.Whh, h_prev) + np.dot(self.Whx, x) + self.bh)\n",
    "    pred = softmax(np.dot(self.Wyh, h_next) + self.by)\n",
    "    return h_next, pred\n",
    "  \n",
    "  def _forward_pass(self, inputs, h):\n",
    "    #h = np.zeros((self.d_h, 1))\n",
    "    hiddens = {}\n",
    "    hiddens[-1] = h\n",
    "    preds = {}\n",
    "    for i in range(len(inputs)):\n",
    "      x_t = inputs[i]\n",
    "      h, pred = self._forward_cell(hiddens[i-1], x_t)\n",
    "      hiddens[i] = h\n",
    "      preds[i] = pred\n",
    "    return hiddens, preds\n",
    "  \n",
    "  \n",
    "  def _backward_cell(self, dh_next, h_prev, h, x_t, p_t, t):\n",
    "    dy = np.copy(p_t)\n",
    "    dy[t] -= 1\n",
    "    dWyh = np.dot(dy, h.T)\n",
    "    dby = dy\n",
    "    dh = np.dot(self.Wyh.T, dy) + dh_next\n",
    "    dtanh = (1 - h ** 2) * dh\n",
    "    dbh = dtanh\n",
    "    dWhx = np.dot(dtanh, x_t.T)\n",
    "    dh_prev = np.dot(self.Whh.T, dtanh)\n",
    "    dWhh = np.dot(dtanh, h_prev.T)\n",
    "    return dh_prev, dWyh, dWhx, dWhh, dby, dbh\n",
    "    \n",
    "  def _backward_pass(self, hiddens, preds, inputs, targets):\n",
    "    dWhx, dWhh, dWyh = np.zeros_like(self.Whx), np.zeros_like(self.Whh), np.zeros_like(self.Wyh)\n",
    "    dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "    dh_next = np.zeros_like(hiddens[0])\n",
    "    for i in reversed(range(len(targets))):\n",
    "      grads = self._backward_cell(dh_next, hiddens[i-1], hiddens[i], inputs[i], preds[i], targets[i])\n",
    "      dh_next = grads[0]\n",
    "      dWyh += grads[1]\n",
    "      dWhx += grads[2]\n",
    "      dWhh += grads[3]\n",
    "      dby += grads[4]\n",
    "      dbh += grads[5]\n",
    "    grads = [dWyh, dWhx, dWhh, dby, dbh]\n",
    "    for param in grads:\n",
    "      np.clip(param, -5, 5, out=param)\n",
    "    return grads\n",
    "    \n",
    "    \n",
    "  def _evaluate(self,inputs, targets, prev_h):\n",
    "    hiddens, preds = self._forward_pass(inputs, prev_h)\n",
    "    loss = 0\n",
    "    for t in range(len(targets)):\n",
    "      loss += -np.log(preds[t][targets[t], 0])\n",
    "    grads = self._backward_pass(hiddens, preds, inputs, targets)\n",
    "      \n",
    "    return loss, hiddens[len(targets)-1], grads\n",
    "  \n",
    "  \n",
    "  def train(self, input_txt):        \n",
    "    t = 0\n",
    "    window = 0\n",
    "    total_loss = 0.\n",
    "    batches = 1\n",
    "    prev_h = np.zeros((hidden_size, 1)) \n",
    "    loss_hist = []\n",
    "    \n",
    "    \n",
    "    while True:      \n",
    "      if batches % 10000 == 0:\n",
    "        print(\"Mean loss so far: \", total_loss/batches)\n",
    "        #if len(loss_hist) > 10 and total_loss/batches > 0.999*loss_hist[-1] and loss_hist[-1] > 0.999*loss_hist[-2] and loss_hist[-2] > 0.999*loss_hist[-3]:\n",
    "        #  break\n",
    "        loss_hist.append(total_loss/batches)\n",
    "        total_loss = 0\n",
    "        batches = 0\n",
    "      \n",
    "      if window+self.seq_len+1 >= len(input_txt):\n",
    "        window = 0\n",
    "        prev_h = np.zeros((hidden_size, 1))\n",
    "        \n",
    "      data = [self.char_ix[ch] for ch in input_txt[window : window + self.seq_len]]\n",
    "      inputs = {}\n",
    "      for t in range(len(data)):\n",
    "        inputs[t] = np.zeros((self.d_v, 1))\n",
    "        inputs[t][data[t]] = 1\n",
    "        \n",
    "      targets = [self.char_ix[ch] for ch in input_txt[window + 1 : window + self.seq_len + 1]]\n",
    "      \n",
    "      \n",
    "      loss, prev_h, grads = self._evaluate(inputs, targets, prev_h)\n",
    "      window += self.seq_len\n",
    "      total_loss += loss\n",
    "      batches += 1\n",
    "      \n",
    "      for param, g, m, v in zip([self.Wyh, self.Whx, self.Whh, self.by, self.bh],\n",
    "                                grads,\n",
    "                                [self.mWyh, self.mWhx, self.mWhh, self.mby, self.mbh],\n",
    "                                [self.vWyh, self.vWhx, self.vWhh, self.vby, self.vbh]):\n",
    "        t+=1\n",
    "        m_prev = m\n",
    "        v_prev = v\n",
    "        m += self.beta1 * m_prev + (1 - self.beta1) * g - m_prev\n",
    "        v += self.beta2 * v_prev + (1 - self.beta2) * np.power(g, 2) - v_prev\n",
    "        m_hat = m / (1 - np.power(self.beta1, t)) + (1 - self.beta1) * g / (1 - np.power(self.beta1, t))\n",
    "        v_hat = v / (1 - np.power(self.beta2, t))\n",
    "        param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "    \n",
    "    print(\"Training Complete!\")\n",
    "    \n",
    "    \n",
    "  def generate(self, sample, sample_size):\n",
    "    sample_input = {}\n",
    "    for i in range(len(sample)-1):\n",
    "      data = np.zeros((self.d_v, 1))\n",
    "      data[self.char_ix[sample[i]]] = 1\n",
    "      sample_input[i] = data\n",
    "    h_prev = np.zeros((self.d_h, 1))\n",
    "    hiddens, preds = self._forward_pass(sample_input, h_prev)\n",
    "    h_prev = hiddens[len(sample_input)-1]\n",
    "  \n",
    "    x_ix = [self.char_ix[ch] for ch in sample[-1]]\n",
    "    x = np.zeros((self.d_v, 1))\n",
    "    x[self.char_ix[sample[-1]]] = 1\n",
    "    sampled = []\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "      h_prev, pred = self._forward_cell(h_prev, x)\n",
    "      pos = np.random.choice(range(self.d_v), p=pred.ravel())\n",
    "      x = np.zeros((self.d_v, 1))\n",
    "      x[pos] = 1\n",
    "      sampled.append(self.ix_char[pos])\n",
    "    \n",
    "    sample_txt = \"\".join(sampled)\n",
    "    print(sample, sample_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q47aMuv7lAnu"
   },
   "outputs": [],
   "source": [
    "data = open('./trial/speeches.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "hidden_size = 200\n",
    "charmap = { ch:i for i,ch in enumerate(chars) }\n",
    "ixmap = { i:ch for i,ch in enumerate(chars) }\n",
    "model = CharRNN(hidden_size, vocab_size, [charmap, ixmap], learning_rate=0.001, seq_len=seq_length)\n",
    "history = model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "unczuXWQju5F",
    "outputId": "fcfc9c4a-9975-40a0-9450-736229ec216a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America . You canâ€™t wait paying to killeh if you can be ablanduy. I think Geved that ban. Hillary trying groral credy â€“ I back free seem now speakise in Tom for a vamberractare issued? You know to do sore and it guy two moner. O\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "These are the Debate, foruse â€“ some people who do this anywhere it come, doan, way rego. Prowes ups contrnor. I was doing out by sought this doesnâ€™t have to do? I took for a raving him byin \"I think way a young from our doll.\" I said they trilling down is tor your manut thing. And we have not see for mbailed to do the many, many of it. Can Brad worata. Itâ€™s sitticks that Hircaistce for a long time both solen. I say the people â€“ he last thing to do so everythical.\n",
      "Clinated it lastens. They said, \"But you want to hambe the history and maybe these competently. Weâ€™re going to rume how states. We did. But she said \"What a ro do, womon.\n",
      "\" for 41% and when do semendless wigh carrina, we swer. I said, \"What have to bone.\n",
      "So this told Trumparatice.\" Iâ€™ll the enstwaria.\n",
      "I love Hurritely bad rutting bankmonk? They donâ€™t go to talk it was beonal thousands, work for Trump says right. But she agration.\n",
      "And weâ€™re going to come in livere was going to cook to anybody and maying about throff one, what drea.\n",
      "95 yances? A should a back.\n",
      "And Iâ€™s doing the veterans disairates? So he Rupectinustileney to Pruz. It they donâ€™t know what the words busised to 91%, I think theyâ€™re earled Toop.\n",
      "Because, so you in do bracts to five much poseres and the should last trea soot sort of num\n"
     ]
    }
   ],
   "source": [
    "model.generate(\"America\", sample_size = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qgXKLMp_NTL",
    "outputId": "94e13343-5ad9-440f-8087-328a2cc15ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your  agoundster a president.\n",
      "You know the veterankly, angry it. Well, I have to do the common From. Weâ€™re great pulitically 45 mangef long in him just military â€“ you know, like some of the end our raidg.\n",
      "I know the kind your was much annories powed in New Hameshing anyther to come in was Bublina, I heard about it as they do. I would beson gent, certements to gever for so taking like them. I think itâ€™s the esticiale. But we talk about anything it with 5% an said, \"Well, of the bolly was success. They donâ€™t lore I donâ€™t know any montinepsest in threartes, wisnor than unementth.\n",
      "And thatâ€™s to making killing to be yourreds of doesnâ€™t suck the Bunzy wrong in. And they didnâ€™t be for a long time, to just walk.\n",
      "So, you atrong it.\n",
      "And thinks about many of coupped forget is very excailly any big laterults. And acroughter.\n",
      "Thereâ€™s some of to funcriviculfralicals â€“ anstee trate debate. But it down. And soec Catcord out in Fox. So many. You know whoâ€™s profors that When Israel doestieve we have to wast to give it but I would sted fibs freen ham put and I so do chinable americh.\n",
      "So we get a fron. I wrialade â€“ any goodent thing sure because I think bohing 200y job, no book, we know low of thes hig. For your over, now itâ€™s more difed. Actually know who wast. Because â€“ he said \"How do you fabe that it sookiolies Trade day divorners.\n",
      "\n",
      "\n",
      "I guess onaring working by the Iâ€™ve nevel sum a plise that Iâ€™m going to do they whething frees of contina? Remember. Itâ€™s the shows.\n",
      "But when you know I was vore wit\n"
     ]
    }
   ],
   "source": [
    "model.generate(\"Your\", sample_size = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rx9GXzAwQsUt",
    "outputId": "36d38f67-56d1-427f-8125-81b3c9f931fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go us become out sole out right?\n",
      "But they have no from Muss advinities.\n",
      "But I wasth? Youâ€™re sord or treates for this and rape turnâ€™t happen. I love.\n",
      "So they didnâ€™t see the same approved a yeat?\" It that stries to stop said Are in Iowa?\" And then we have to expelsed and just wasnâ€™t have to use that could bean in the other thinkwork â€“ I took who it were sort of money to or build. Will mess it would have see hupe in bad? I said \"These in Scumings to I was think itâ€™s the Bingunes on heatinest better good jobs. I love Trump. I heper and everybody.\n",
      "Now, alowere buildicelably rediticals our scement.\n",
      "You know, the other dived, the Deal Hillary in only been was gine and to do. And I scates. They using Aur. Plints this day\" Some of the White Horved in the Alankloy.\n",
      "Mark our counsing your loppanics. When I soot.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The stopa like. But I say they looking mouth was gaiting History at the cozrane.\n",
      "My your dourast to the America Iobalie than stregite, and then you know Mexico. When Ickionips you reel â€“ a vero hard to do whill so Naptricted fuy they no with whates to call it. Weâ€™re doing so special.\n",
      "And $200,000 big the finh â€“ money our cape and his I have under work in the money suaned time it so tremendoust.\n",
      " Tobde goad including bad amount.\" They believe me, these cits. The world times. You know the hotel Sandies\". Itâ€™s this doengs.\n",
      "So Doray: So, so we or a suge antifully heâ€™s great. And I undenstoness â€“ the Wall â€“ heâ€™s going to chat happening. And we have a supposed in While and hit do o\n"
     ]
    }
   ],
   "source": [
    "model.generate(\"Go\", sample_size = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30Qi43YvQsUv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2-2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
